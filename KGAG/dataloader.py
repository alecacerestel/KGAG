"""
Data Integration Module: BeHAVE Output to KGAG Input

This module loads the output files generated by BeHAVE and converts them
into the format required by KGAG model.

BeHAVE Output Files:
- groupMember.txt: Group membership information
- groupRatingTrain.txt: Group-item interactions (positive samples)
- userRatingTrain.txt: User-item interactions
- kg_final.txt: Knowledge graph triples (head, relation, tail)
- item_embeddings_final.npy: Pre-trained item embeddings (optional)

KGAG Input Requirements:
- user_item_edges: Tensor (2, num_edges) with [user_ids, item_ids]
- kg_edges: Tensor (2, num_edges) with [head_entity, tail_entity]
- group_members: Dictionary mapping group_id to list of user_ids
- positive/negative samples for training (interactions positive with people is positive, negative sampling for negatives)

Usage:
from KGAG.dataloader import load_behave_data

    loader = load_behave_data('dataset/MovieLens_RecBole_KG/BeHAVE/')
    user_item_edges = loader.get_user_item_edges()
    kg_edges = loader.get_kg_edges()
    groups = loader.get_group_members()

- test it with: python KGAG/dataloader.py
"""

import os
import numpy as np
import torch
from collections import defaultdict
from torch.utils.data import TensorDataset, DataLoader


class BeHAVEDataLoader:
    """
    Loads BeHAVE output files and converts them to KGAG input format.
    
    Usage:
        loader = BeHAVEDataLoader(data_dir='dataset/MovieLens_RecBole_KG/BeHAVE/')
        loader.load_all()
        
        # Get data for KGAG
        user_item_edges = loader.get_user_item_edges()
        kg_edges = loader.get_kg_edges()
        groups = loader.get_group_members()
    """
    
    def __init__(self, data_dir):
        """
        Initialize data loader.
        
        Args:
            data_dir: Path to BeHAVE output directory
        """
        self.data_dir = data_dir
        
        # Data storage
        self.user_item_interactions = []  # List of (user_id, item_id) tuples
        self.group_item_interactions = []  # List of (group_id, item_id) tuples
        self.group_members = {}  # Dict: group_id -> list of user_ids
        self.kg_triples = []  # List of (head, relation, tail) tuples
        
        # Mappings (original ID -> internal index)
        self.user_id_map = {}
        self.item_id_map = {}
        self.entity_id_map = {}
        self.relation_id_map = {}
        self.group_id_map = {}
        
        # Statistics
        self.num_users = 0
        self.num_items = 0
        self.num_entities = 0
        self.num_relations = 0
        self.num_groups = 0
    
    def load_all(self):
        """
        Load all BeHAVE output files.
        
        Call this method to load all data at once.
        """
        print("Loading BeHAVE output files...")
        
        self.load_user_item_interactions()
        self.load_group_members()
        self.load_group_item_interactions()
        self.load_knowledge_graph()
        
        self._print_statistics()
    
    def load_user_item_interactions(self):
        """
        Load user-item interactions from userRatingTrain.txt.
        
        File format: user_id item_id rating
        Example: 0 42 5.0
        """
        file_path = os.path.join(self.data_dir, 'userRatingTrain.txt')
        
        if not os.path.exists(file_path):
            print(f"Warning: {file_path} not found. Skipping user-item interactions.")
            return
        
        print(f"Loading user-item interactions from {file_path}...")
        
        with open(file_path, 'r') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                
                parts = line.split()
                if len(parts) < 2:
                    continue
                
                user_id = int(parts[0])
                item_id = int(parts[1])
                
                # Create mappings
                if user_id not in self.user_id_map:
                    self.user_id_map[user_id] = self.num_users
                    self.num_users += 1
                
                if item_id not in self.item_id_map:
                    self.item_id_map[item_id] = self.num_items
                    self.num_items += 1
                
                # Store interaction
                internal_user = self.user_id_map[user_id]
                internal_item = self.item_id_map[item_id]
                self.user_item_interactions.append((internal_user, internal_item))
        
        print(f"  Loaded {len(self.user_item_interactions)} user-item interactions")
        print(f"  Users: {self.num_users}, Items: {self.num_items}")
    
    # POTENTIEL_PB
    def load_group_members(self):
        """
        Load group membership from groupMember.txt.
        
        File format: group_id user_id_1 user_id_2 ... user_id_n
        Example: 0 5 12 23 31
        """
        file_path = os.path.join(self.data_dir, 'groupMember.txt')
        
        if not os.path.exists(file_path):
            print(f"Warning: {file_path} not found. Skipping group members.")
            return
        
        print(f"Loading group members from {file_path}...")
        
        with open(file_path, 'r') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                
                parts = line.split()
                if len(parts) < 2:
                    continue
                
                group_id = int(parts[0])
                parts[1:] = parts[1:][0].split(',')
                member_ids = [int(uid) for uid in parts[1:]]
                
                # Create group mapping
                if group_id not in self.group_id_map:
                    self.group_id_map[group_id] = self.num_groups
                    self.num_groups += 1
                
                internal_group = self.group_id_map[group_id]
                
                # Map member IDs to internal user IDs
                internal_members = []
                for uid in member_ids:
                    if uid in self.user_id_map:
                        internal_members.append(self.user_id_map[uid])
                    else:
                        # User not in training data, skip
                        continue
                
                if internal_members:
                    self.group_members[internal_group] = internal_members
        
        print(f"  Loaded {len(self.group_members)} groups")
        avg_size = np.mean([len(members) for members in self.group_members.values()])
        print(f"  Average group size: {avg_size:.2f}")
    
    def load_group_item_interactions(self):
        """
        Load group-item interactions from groupRatingTrain.txt.
        
        File format: group_id item_id rating
        Example: 0 42 4.5
        """
        file_path = os.path.join(self.data_dir, 'groupRatingTrain.txt')
        
        if not os.path.exists(file_path):
            print(f"Warning: {file_path} not found. Skipping group-item interactions.")
            return
        
        print(f"Loading group-item interactions from {file_path}...")
        
        with open(file_path, 'r') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                
                parts = line.split()
                if len(parts) < 2:
                    continue
                
                group_id = int(parts[0])
                item_id = int(parts[1])
                
                # Map to internal IDs
                if group_id not in self.group_id_map:
                    continue
                if item_id not in self.item_id_map:
                    continue
                
                internal_group = self.group_id_map[group_id]
                internal_item = self.item_id_map[item_id]
                
                self.group_item_interactions.append((internal_group, internal_item))
        
        print(f"  Loaded {len(self.group_item_interactions)} group-item interactions")
    
    def load_knowledge_graph(self):
        """
        Load knowledge graph from kg_final.txt.
        
        File format: head_id relation_id tail_id
        Example: 42 0 1337
        
        Note: Entities in KG include items plus additional entities (actors, genres, etc.)
        Items are mapped to entities by their IDs.
        """
        file_path = os.path.join(self.data_dir, '../kg_final.txt')
        
        if not os.path.exists(file_path):
            print(f"Warning: {file_path} not found. Skipping knowledge graph.")
            return
        
        print(f"Loading knowledge graph from {file_path}...")
        
        # First pass: map items to entities
        # Items that exist in user-item interactions should be entities
        for item_id in self.item_id_map.keys():
            if item_id not in self.entity_id_map:
                self.entity_id_map[item_id] = self.num_entities
                self.num_entities += 1
        
        # Second pass: load KG triples
        with open(file_path, 'r') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                
                parts = line.split()
                if len(parts) < 3:
                    continue
                
                head = int(parts[0])
                relation = int(parts[1])
                tail = int(parts[2])
                
                # Map entities
                if head not in self.entity_id_map:
                    self.entity_id_map[head] = self.num_entities
                    self.num_entities += 1
                
                if tail not in self.entity_id_map:
                    self.entity_id_map[tail] = self.num_entities
                    self.num_entities += 1
                
                # Map relations
                if relation not in self.relation_id_map:
                    self.relation_id_map[relation] = self.num_relations
                    self.num_relations += 1
                
                # Store triple
                internal_head = self.entity_id_map[head]
                internal_relation = self.relation_id_map[relation]
                internal_tail = self.entity_id_map[tail]
                
                self.kg_triples.append((internal_head, internal_relation, internal_tail))
        
        print(f"  Loaded {len(self.kg_triples)} KG triples")
        print(f"  Entities: {self.num_entities}, Relations: {self.num_relations}")
    
    def get_user_item_edges(self):
        """
        Get user-item edges as PyTorch tensor for KGAG.
        
        Returns:
            user_item_edges: Tensor of shape (2, num_edges)
                            [0, :] = user indices
                            [1, :] = item indices (mapped to entity space if KG exists, otherwise item indices)
        """
        if not self.user_item_interactions:
            return torch.empty((2, 0), dtype=torch.long)
        
        users = []
        items = []
        
        for user, item in self.user_item_interactions:
            users.append(user)
            # If we have a knowledge graph, map items to entity space
            # Otherwise, just use the item indices directly
            if self.entity_id_map:
                original_item_id = [k for k, v in self.item_id_map.items() if v == item][0]
                entity_idx = self.entity_id_map[original_item_id]
                items.append(entity_idx)
            else:
                items.append(item)
        
        user_item_edges = torch.tensor([users, items], dtype=torch.long)
        return user_item_edges
    
    def get_kg_edges(self):
        """
        Get knowledge graph edges as PyTorch tensor for KGAG.
        
        Returns:
            kg_edges: Tensor of shape (2, num_edges)
                     [0, :] = head entity indices
                     [1, :] = tail entity indices
        """
        if not self.kg_triples:
            return torch.empty((2, 0), dtype=torch.long)
        
        heads = [triple[0] for triple in self.kg_triples]
        tails = [triple[2] for triple in self.kg_triples]
        
        kg_edges = torch.tensor([heads, tails], dtype=torch.long)
        return kg_edges
    
    def get_kg_edges_with_relations(self):
        """
        Get knowledge graph edges with relation types.
        
        Returns:
            edge_index: Tensor of shape (2, num_edges) with [head, tail]
            edge_type: Tensor of shape (num_edges,) with relation types
        """
        if not self.kg_triples:
            return torch.empty((2, 0), dtype=torch.long), torch.empty(0, dtype=torch.long)
        
        heads = [triple[0] for triple in self.kg_triples]
        relations = [triple[1] for triple in self.kg_triples]
        tails = [triple[2] for triple in self.kg_triples]
        
        edge_index = torch.tensor([heads, tails], dtype=torch.long)
        edge_type = torch.tensor(relations, dtype=torch.long)
        
        return edge_index, edge_type
    
    def get_group_members(self):
        """
        Get group membership dictionary.
        
        Returns:
            group_members: Dictionary mapping group_id to list of user_ids
        """
        return self.group_members
    
    
    # create two new functions, one for user, a second for groups,
    def get_training_samples(self, num_negatives=1):
        """
        Generate training samples for KGAG with negative sampling
        
        Args:
            num_negatives: Number of negative items per positive sample
        
        Returns:
            positive_samples: List of (group_id, item_id) tuples
            negative_samples: List of (group_id, item_id) tuples (num_negatives per positive)
        """
        positive_samples = self.group_item_interactions.copy()
        negative_samples = []
        
        # Build set of positive items per group for efficient lookup
        group_positive_items = defaultdict(set)
        for group_id, item_id in positive_samples:
            group_positive_items[group_id].add(item_id)
        
        # Generate negative samples
        all_items = list(range(self.num_items))
        
        for group_id, pos_item_id in positive_samples:
            # Sample negative items that the group has not interacted with
            
            # POTENTIAL_PB
            group_positives = group_positive_items[group_id]
            
            neg_count = 0
            attempts = 0
            max_attempts = num_negatives * 100
            
            while neg_count < num_negatives and attempts < max_attempts:
                neg_item = np.random.choice(all_items)
                
                # Map to entity space
                original_item_id = [k for k, v in self.item_id_map.items() if v == neg_item][0]
                entity_idx = self.entity_id_map[original_item_id]
                
                if entity_idx not in group_positives:
                    negative_samples.append((group_id, entity_idx))
                    neg_count += 1
                
                attempts += 1
        
        return positive_samples, negative_samples
    
    def get_user_training_samples(self, num_negatives=1):
        """
        Generate training samples for users with negative sampling
        
        Args:
            num_negatives: Number of negative items per positive sample
        
        Returns:
            positive_samples: List of (user_id, item_id) tuples
            negative_samples: List of (user_id, item_id) tuples (num_negatives per positive)
        """
        positive_samples = self.user_item_interactions.copy()
        negative_samples = []
        
        # Build set of positive items per user for efficient lookup
        user_positive_items = defaultdict(set)
        for user_id, item_id in positive_samples:
            user_positive_items[user_id].add(item_id)
        
        # Generate negative samples
        all_items = list(range(self.num_items))
        
        for user_id, pos_item_id in positive_samples:
            # Sample negative items that the user has not interacted with
            user_positives = user_positive_items[user_id]
            
            neg_count = 0
            attempts = 0
            max_attempts = num_negatives * 100
            
            while neg_count < num_negatives and attempts < max_attempts:
                neg_item = np.random.choice(all_items)
                
                # Map to entity space if KG exists
                if self.entity_id_map:
                    original_item_id = [k for k, v in self.item_id_map.items() if v == neg_item][0]
                    entity_idx = self.entity_id_map[original_item_id]
                else:
                    entity_idx = neg_item
                
                if entity_idx not in user_positives:
                    negative_samples.append((user_id, entity_idx))
                    neg_count += 1
                
                attempts += 1
        
        return positive_samples, negative_samples

    def get_batches(self, batch_size, mode='group', num_negatives=1, shuffle=True):
        """
        Generate batches of training samples for users or groups.
        
        Args:
            batch_size: Number of samples per batch
            mode: Either 'user' or 'group' to specify which training samples to use
            num_negatives: Number of negative items per positive sample
            shuffle: Whether to shuffle samples before batching
        
        Returns:
            DataLoader: PyTorch DataLoader for iterating over batches
        """
        
        # Get training samples based on mode
        if mode == 'user':
            pos_samples, neg_samples = self.get_user_training_samples(num_negatives)
        elif mode == 'group':
            pos_samples, neg_samples = self.get_training_samples(num_negatives)
        else:
            raise ValueError(f"Invalid mode '{mode}'. Must be 'user' or 'group'.")
        
        # Create triplets (id, positive_item, negative_item)
        triplets = []
        for i, (entity_id, pos_item) in enumerate(pos_samples):
            for j in range(num_negatives):
                neg_idx = i * num_negatives + j
                if neg_idx < len(neg_samples):
                    _, neg_item = neg_samples[neg_idx]
                    triplets.append([entity_id, pos_item, neg_item])
        
        # Convert to tensor
        triplets_tensor = torch.tensor(triplets, dtype=torch.long)
        
        # Create TensorDataset
        dataset = TensorDataset(triplets_tensor)
        
        # Create and return DataLoader
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            drop_last=False
        )
        
        return dataloader
    
    def load_item_embeddings(self):
        """
        Load pre-trained item embeddings from item_embeddings_final.npy.
        
        Returns:
            embeddings: numpy array of shape (num_items, embedding_dim)
        """
        file_path = os.path.join(self.data_dir, 'item_embeddings_final.npy')
        
        if not os.path.exists(file_path):
            print(f"Warning: {file_path} not found.")
            return None
        
        print(f"Loading item embeddings from {file_path}...")
        embeddings = np.load(file_path)
        print(f"  Loaded embeddings with shape: {embeddings.shape}")
        
        return embeddings
    
    def load_test_data(self):
        """
        Load test data from groupRatingTest.txt and userRatingTest.txt.
        
        Returns:
            test_groups: List of (group_id, item_id) tuples
            test_users: List of (user_id, item_id) tuples
        """
        test_groups = []
        test_users = []
        
        # Load group test data
        group_test_path = os.path.join(self.data_dir, 'groupRatingTest.txt')
        if os.path.exists(group_test_path):
            print(f"Loading test groups from {group_test_path}...")
            with open(group_test_path, 'r') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    parts = line.split()
                    if len(parts) >= 2:
                        group_id = int(parts[0])
                        item_id = int(parts[1])
                        
                        if group_id in self.group_id_map and item_id in self.item_id_map:
                            test_groups.append((
                                self.group_id_map[group_id],
                                self.item_id_map[item_id]
                            ))
            print(f"  Loaded {len(test_groups)} test group interactions")
        
        # Load user test data
        user_test_path = os.path.join(self.data_dir, 'userRatingTest.txt')
        if os.path.exists(user_test_path):
            print(f"Loading test users from {user_test_path}...")
            with open(user_test_path, 'r') as f:
                for line in f:
                    line = line.strip()
                    if not line:
                        continue
                    parts = line.split()
                    if len(parts) >= 2:
                        user_id = int(parts[0])
                        item_id = int(parts[1])
                        
                        if user_id in self.user_id_map and item_id in self.item_id_map:
                            test_users.append((
                                self.user_id_map[user_id],
                                self.item_id_map[item_id]
                            ))
            print(f"  Loaded {len(test_users)} test user interactions")
        
        return test_groups, test_users
    
    def get_test_groups(self):
        """
        Get test group interactions.
        
        Returns:
            test_groups: List of (group_id, item_id) tuples
        """
        test_groups, _ = self.load_test_data()
        return test_groups

    def _print_statistics(self):
        """Print data statistics."""
        
        print("Data Statistics:")
        print(f"Users: {self.num_users}")
        print(f"Items: {self.num_items}")
        print(f"Entities: {self.num_entities}")
        print(f"Relations: {self.num_relations}")
        print(f"Groups: {self.num_groups}")
        print(f"User-Item interactions: {len(self.user_item_interactions)}")
        print(f"Group-Item interactions: {len(self.group_item_interactions)}")
        print(f"KG triples: {len(self.kg_triples)}")
        
        if self.group_members:
            sizes = [len(members) for members in self.group_members.values()]
            print(f"Group size: min={min(sizes)}, max={max(sizes)}, avg={np.mean(sizes):.2f}")

def load_behave_data(data_dir):
    """
    Convenience function to load BeHAVE data in one call.
    
    Args:
        data_dir: Path to BeHAVE output directory
    
    Returns:
        loader: BeHAVEDataLoader instance with loaded data
    """
    loader = BeHAVEDataLoader(data_dir)
    loader.load_all()
    return loader


if __name__ == '__main__':
    """
    Test the data loader
    """
    # Example usage
    data_dir = 'dataset/MovieLens_RecBole_KG/BeHAVE/'
    
    print("Testing BeHAVE Data Loader")
    print("=" * 60)
    
    loader = load_behave_data(data_dir)
    
    # Get data for KGAG
    user_item_edges = loader.get_user_item_edges()
    kg_edges = loader.get_kg_edges()
    groups = loader.get_group_members()
    
    print("\nKGAG Input Format:")
    print(f"user_item_edges shape: {user_item_edges.shape}")
    print(f"kg_edges shape: {kg_edges.shape}")
    print(f"Number of groups: {len(groups)}")
    
    # Sample a group
    if groups:
        sample_group_id = list(groups.keys())[0]
        sample_members = groups[sample_group_id]
        print(f"\nSample Group {sample_group_id}:")
        print(f"  Members: {sample_members}")
        print(f"  Size: {len(sample_members)}")
    
    # Generate training samples
    print("\nGenerating training samples...")
    pos_samples, neg_samples = loader.get_training_samples(num_negatives=2)
    print(f"Positive samples: {len(pos_samples)}")
    print(f"Negative samples: {len(neg_samples)}")
    
    print("\nData loading complete.")
